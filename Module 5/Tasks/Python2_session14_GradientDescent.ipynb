{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 14: Gradient Descent for Multivariate Linear Regression\n",
    "\n",
    "In this session, we will:\n",
    "1. Learn the basics of **NumPy** for numerical computing\n",
    "2. Understand **gradient descent** optimization\n",
    "3. Implement **multivariate linear regression** from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Introduction to NumPy\n",
    "\n",
    "NumPy is the fundamental package for numerical computing in Python. It provides:\n",
    "- N-dimensional arrays (`ndarray`)\n",
    "- Broadcasting for element-wise operations\n",
    "- Linear algebra operations\n",
    "- Mathematical functions optimized for arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Creating Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1D array: [-1  2  3  4  5]\n",
      "Shape: (5,)\n",
      "Dtype: int8\n"
     ]
    }
   ],
   "source": [
    "# From Python lists\n",
    "arr1 = np.array([-1, 2, 3, 4, 5], dtype=np.int8)\n",
    "print(f\"1D array: {arr1}\")\n",
    "print(f\"Shape: {arr1.shape}\")\n",
    "print(f\"Dtype: {arr1.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D array:\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "Shape: (2, 3)\n"
     ]
    }
   ],
   "source": [
    "# 2D array (matrix)\n",
    "arr2 = np.array([[1, 2, 3], \n",
    "                 [4, 5, 6]])\n",
    "print(f\"2D array:\\n{arr2}\")\n",
    "print(f\"Shape: {arr2.shape}\")  # (rows, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zeros:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "ones:\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "\n",
      "random:\n",
      "[[-1.52533979 -0.34456174 -0.26328669]\n",
      " [-0.3866013   1.39683547  0.02544633]\n",
      " [-0.91252505  0.31617937 -3.55318463]]\n",
      "\n",
      "arange: [0 2 4 6 8]\n",
      "linspace: [0.   0.25 0.5  0.75 1.  ]\n"
     ]
    }
   ],
   "source": [
    "# Common array creation functions\n",
    "zeros = np.zeros((3, 4))       # 3x4 matrix of zeros\n",
    "ones = np.ones((2, 3))         # 2x3 matrix of ones\n",
    "random = np.random.randn(3, 3) # 3x3 matrix of random values (normal distribution)\n",
    "arange = np.arange(0, 10, 2)   # [0, 2, 4, 6, 8]\n",
    "linspace = np.linspace(0, 1, 5) # 5 evenly spaced values from 0 to 1\n",
    "\n",
    "print(f\"zeros:\\n{zeros}\\n\")\n",
    "print(f\"ones:\\n{ones}\\n\")\n",
    "print(f\"random:\\n{random}\\n\")\n",
    "print(f\"arange: {arange}\")\n",
    "print(f\"linspace: {linspace}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Array Operations and Broadcasting\n",
    "\n",
    "NumPy operations are **element-wise** by default. Broadcasting allows operations between arrays of different shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a + b = [5 7 9]\n",
      "a * b = [ 4 10 18]\n",
      "a ** 2 = [1 4 9]\n",
      "a * 10 = [10 20 30]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "\n",
    "print(f\"a + b = {a + b}\")      # Element-wise addition\n",
    "print(f\"a * b = {a * b}\")      # Element-wise multiplication\n",
    "print(f\"a ** 2 = {a ** 2}\")    # Element-wise power\n",
    "print(f\"a * 10 = {a * 10}\")    # Broadcasting: scalar applied to all elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix + row_vector:\n",
      "[[11 22 33]\n",
      " [14 25 36]]\n"
     ]
    }
   ],
   "source": [
    "# Broadcasting with 2D arrays\n",
    "matrix = np.array([[1, 2, 3],\n",
    "                   [4, 5, 6]])\n",
    "row_vector = np.array([10, 20, 30])\n",
    "\n",
    "\"\"\"\n",
    "[[10, 20, 30],\n",
    "[10, 20 ,30]]\n",
    "\"\"\"\n",
    "\n",
    "# The row vector is \"broadcast\" to each row of the matrix\n",
    "result = matrix + row_vector\n",
    "print(f\"Matrix + row_vector:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Matrix Operations\n",
    "\n",
    "For linear regression, we need matrix multiplication and transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A @ B (matrix multiplication):\n",
      "[[19 22]\n",
      " [43 50]]\n",
      "\n",
      "np.dot(A, B):\n",
      "[[19 22]\n",
      " [43 50]]\n",
      "\n",
      "A.T (transpose):\n",
      "[[1 3]\n",
      " [2 4]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 2], \n",
    "              [3, 4]])\n",
    "B = np.array([[5, 6], \n",
    "              [7, 8]])\n",
    "\n",
    "# Matrix multiplication (dot product)\n",
    "print(f\"A @ B (matrix multiplication):\\n{A @ B}\")\n",
    "print(f\"\\nnp.dot(A, B):\\n{np.dot(A, B)}\")\n",
    "\n",
    "# Transpose\n",
    "print(f\"\\nA.T (transpose):\\n{A.T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Useful Aggregation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum all: 21\n",
      "Sum along rows (axis=1): [ 6 15]\n",
      "Sum along columns (axis=0): [5 7 9]\n",
      "Mean: 3.5\n",
      "Std: 1.707825127659933\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[1, 2, 3],\n",
    "                [4, 5, 6]])\n",
    "\n",
    "print(f\"Sum all: {np.sum(arr)}\")\n",
    "print(f\"Sum along rows (axis=1): {np.sum(arr, axis=1)}\")\n",
    "print(f\"Sum along columns (axis=0): {np.sum(arr, axis=0)}\")\n",
    "print(f\"Mean: {np.mean(arr)}\")\n",
    "print(f\"Std: {np.std(arr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Linear Regression Review\n",
    "\n",
    "### What is Linear Regression?\n",
    "\n",
    "Linear regression models the relationship between features $X$ and target $y$ as:\n",
    "\n",
    "$$\\hat{y} = X \\cdot w + b$$\n",
    "\n",
    "Where:\n",
    "- $X$ is the feature matrix of shape `(n_samples, n_features)`\n",
    "- $w$ is the weight vector of shape `(n_features,)`\n",
    "- $b$ is the bias (intercept) scalar\n",
    "- $\\hat{y}$ is the predicted values\n",
    "\n",
    "### The Goal\n",
    "\n",
    "Find $w$ and $b$ that minimize the **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$\\mathcal{L} = MSE = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y_i} - y_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients for Linear Regression\n",
    "\n",
    "For MSE loss with linear regression, the gradients are:\n",
    "\n",
    "$$\\nabla_w \\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial w} = \\frac{\\partial MSE}{\\partial w} = \\frac{2}{n}  (\\hat{y} - y) X$$\n",
    "\n",
    "$$\\nabla_b \\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{\\partial MSE}{\\partial b} = \\frac{2}{n} \\sum(\\hat{y} - y)$$\n",
    "\n",
    "Let's derive this step by step:\n",
    "\n",
    "1. $MSE = \\frac{1}{n}\\sum(\\hat{y} - y)^2 = \\frac{1}{n}\\sum(Xw + b - y)^2$\n",
    "\n",
    "2. Using chain rule: $\\frac{\\partial MSE}{\\partial w} = \\frac{2}{n} \\cdot (Xw + b - y) \\cdot X$\n",
    "\n",
    "3. In matrix form: $\\frac{\\partial MSE}{\\partial w} = \\frac{2}{n} (\\hat{y} - y) X$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Gradient Descent\n",
    "\n",
    "### What is Gradient Descent?\n",
    "\n",
    "Gradient descent is an optimization algorithm that iteratively updates parameters to minimize a loss function.\n",
    "\n",
    "Think of it like descending a mountain in fog - you can only feel the slope at your current position, so you take small steps in the steepest downward direction.\n",
    "\n",
    "### The Update Rule\n",
    "\n",
    "$$w_{new} = w_{old} - \\alpha \\cdot \\nabla_w \\mathcal{L} = w_{old} - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial w} $$\n",
    "\n",
    "Where $\\alpha$ is the **learning rate** - how big of a step we take."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Implementation from Scratch\n",
    "\n",
    "Let's build our linear regression step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1000, 3)\n",
      "y shape: (1000,)\n",
      "True weights: [ 2.  -3.5  1.5]\n",
      "True bias: 5.0\n"
     ]
    }
   ],
   "source": [
    "# First, let's create some synthetic data\n",
    "np.random.seed(42)\n",
    "\n",
    "# True parameters we want to learn\n",
    "true_weights = np.array([2.0, -3.5, 1.5])\n",
    "true_bias = 5.0\n",
    "\n",
    "# Generate random features\n",
    "n_samples = 1000\n",
    "n_features = 3\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "\n",
    "# Generate target with some noise\n",
    "noise = np.random.randn(n_samples) * 0.5\n",
    "y = X @ true_weights + true_bias + noise\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"True weights: {true_weights}\")\n",
    "print(f\"True bias: {true_bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Core Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-class Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write `predict()`, `compute_mse()`, `compute_gradients()`, which perform neccessary operations for gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X: np.ndarray, w: np.ndarray, b: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute predictions for linear regression.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features)\n",
    "        w: Weight vector of shape (n_features,)\n",
    "        b: Bias scalar\n",
    "    \n",
    "    Returns:\n",
    "        Predictions of shape (n_samples,)\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    return (X @ w) + b   # y = X dot w + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute Mean Squared Error.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Actual target values\n",
    "        y_pred: Predicted values\n",
    "    \n",
    "    Returns:\n",
    "        MSE loss value\n",
    "    \"\"\"\n",
    "   # Your code here\n",
    "    n = len(y_true)\n",
    "    return np.sum((y_pred - y_true) ** 2) / n\n",
    "    # return np.mean(np.sum((y_pred - y_true) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(X: np.ndarray, y: np.ndarray, y_pred: np.ndarray) -> tuple:\n",
    "    \"\"\"\n",
    "    Compute gradients for weights and bias.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features)\n",
    "        y: True target values\n",
    "        y_pred: Predicted values\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (gradient_w, gradient_b)\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    n = X.shape[0] # len(y), len(X), y.shape(0)\n",
    "    error = y_pred - y # (n,)\n",
    "\n",
    "    grad_w = (2/n) * (X.T @ error) # shape (n_features,)\n",
    "    grad_b = (2/n) * np.sum(error) # scalar\n",
    "\n",
    "    return grad_w, grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute first gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's create some synthetic data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate random features\n",
    "n_samples = 1000\n",
    "n_features = 3\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "\n",
    "# True parameters we want to learn\n",
    "true_weights = np.array([2.0, -3.5, 1.5])\n",
    "true_bias = 5.0\n",
    "\n",
    "# Generate y with noise\n",
    "noise = np.random.randn(n_samples) * 0.5\n",
    "y = X @ true_weights + true_bias + noise\n",
    "\n",
    "w = np.random.rand(3)\n",
    "b = np.array([0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 3), (3,), (1,))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, w.shape, b.shape # Should be ((1000, 3), (3,), (1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-3.73806935,  7.47985026, -1.14952725]), np.float64(-10.240308027253475))\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(X, w, b) # Initial predictions\n",
    "print(compute_gradients(X, y, y_pred)) # Should print gradients (grad_w, grad_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you observe what happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_regression(\n",
    "    X: np.ndarray, \n",
    "    y: np.ndarray, \n",
    "    learning_rate: float = 0.01, \n",
    "    n_iterations: int = 1000,\n",
    "    verbose: bool = True,\n",
    "    log_every_n_step = 25,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train linear regression using gradient descent.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features)\n",
    "        y: Target values of shape (n_samples,)\n",
    "        learning_rate: Step size for gradient descent\n",
    "        n_iterations: Number of training iterations\n",
    "        verbose: Whether to print progress\n",
    "        log_every_n_step: Number of steps to log the result\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (final_weights, final_bias, loss_history)\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Initialize parameters randomly\n",
    "    w = np.random.randn(n_features) * 0.01\n",
    "    b = 0.0\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Your code here\n",
    "        pass\n",
    "        \n",
    "        if verbose and (i % log_every_n_step == 0 or i == n_iterations - 1):\n",
    "            print(f\"Iteration {i:4d} | Loss: {loss:.6f}\")\n",
    "    \n",
    "    return w, b, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m learned_w, learned_b, losses = \u001b[43mtrain_linear_regression\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_iterations\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mtrain_linear_regression\u001b[39m\u001b[34m(X, y, learning_rate, n_iterations, verbose, log_every_n_step)\u001b[39m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;129;01mand\u001b[39;00m (i % log_every_n_step == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i == n_iterations - \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m4d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mloss\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m w, b, loss_history\n",
      "\u001b[31mNameError\u001b[39m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "learned_w, learned_b, losses = train_linear_regression(\n",
    "    X, y, \n",
    "    learning_rate=0.01, \n",
    "    n_iterations=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare learned parameters with true parameters\n",
    "print(\"\\n=== Results ===\")\n",
    "print(f\"True weights:    {true_weights}\")\n",
    "print(f\"Learned weights: {learned_w}\")\n",
    "print(f\"\\nTrue bias:    {true_bias}\")\n",
    "print(f\"Learned bias: {learned_b:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the loss curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training Loss over Time')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Feature Scaling\n",
    "\n",
    "Gradient descent works much better when features are on similar scales. Let's see why and how to fix it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is statistical way to standarize any sameple of feature $i$ to a normal distribution:\n",
    "$$X_i = \\frac{x_i-\\mu_i}{\\sigma_i}$$\n",
    "where $X_i$ is the column vector of raw data feature $i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with very different scales\n",
    "np.random.seed(42)\n",
    "X_unscaled = np.column_stack([\n",
    "    np.random.randn(500) * 1000,      # Feature 1: scale ~1000\n",
    "    np.random.randn(500) * 0.001,     # Feature 2: scale ~0.001\n",
    "    np.random.randn(500)              # Feature 3: scale ~1\n",
    "])\n",
    "y_unscaled = X_unscaled @ np.array([0.001, 1000, 1]) + 5 + np.random.randn(500) * 0.5\n",
    "\n",
    "print(f\"Feature ranges:\")\n",
    "for i in range(3):\n",
    "    print(f\"  Feature {i+1}: [{X_unscaled[:, i].min():.2f}, {X_unscaled[:, i].max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will likely fail or converge very slowly!\n",
    "try:\n",
    "    w_bad, b_bad, losses_bad = train_linear_regression(\n",
    "        X_unscaled, y_unscaled, learning_rate=0.01, n_iterations=100\n",
    "    )\n",
    "except:\n",
    "    print(\"Training failed due to numerical instability!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Implement standardize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Standardization (Z-score normalization)\n",
    "def standardize(X: np.ndarray) -> tuple:\n",
    "    \"\"\"\n",
    "    Standardize features to have mean=0 and std=1.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (X_standardized, mean, std)\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    mean = X.mean(axis=0)     # (n_features,)\n",
    "    std = X.std(axis=0)       # (n_features,)\n",
    "    \n",
    "    # Prevent division by zero\n",
    "    std[std == 0] = 1\n",
    "    \n",
    "    X_standardized = (X - mean) / std\n",
    "    return X_standardized, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_unscaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Standardize and train\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m X_scaled, X_mean, X_std = standardize(\u001b[43mX_unscaled\u001b[49m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mScaled feature ranges:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m3\u001b[39m):\n",
      "\u001b[31mNameError\u001b[39m: name 'X_unscaled' is not defined"
     ]
    }
   ],
   "source": [
    "# Standardize and train\n",
    "X_scaled, X_mean, X_std = standardize(X_unscaled)\n",
    "\n",
    "print(f\"Scaled feature ranges:\")\n",
    "for i in range(3):\n",
    "    print(f\"  Feature {i+1}: [{X_scaled[:, i].min():.2f}, {X_scaled[:, i].max():.2f}]\")\n",
    "\n",
    "w_good, b_good, losses_good = train_linear_regression(\n",
    "    X_scaled, y_unscaled, learning_rate=0.1, n_iterations=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Tasks (Deadline: Sunday 30th Nov 2025)\n",
    "\n",
    "Complete the following tasks to practice implementing gradient descent for linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Implement Mini-Batch Gradient Descent\n",
    "\n",
    "Instead of using all samples in each iteration (batch gradient descent), implement **mini-batch gradient descent** which uses a random subset of samples.\n",
    "\n",
    "Formally said, choose $X_b$ and its corresponding $y_b$ which is a subset of $row(X), row(y)$ to be trained for each iteration.\n",
    "\n",
    "\n",
    "Benefits of mini-batch:\n",
    "- Faster iterations\n",
    "- Can escape local minima\n",
    "- Better generalization\n",
    "\n",
    "```python\n",
    "# Expected usage:\n",
    "w, b, losses = train_minibatch_gd(X, y, batch_size=32, learning_rate=0.01, n_iterations=1000)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_minibatch_gd(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    batch_size: int = 32,\n",
    "    learning_rate: float = 0.01,\n",
    "    n_iterations: int = 1000,\n",
    "    verbose: bool = True,\n",
    "    log_every_n_step: int = 20,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train linear regression using mini-batch gradient descent.\n",
    "    \n",
    "    Hints:\n",
    "    - Use np.random.choice to select random indices\n",
    "    - Compute gradients using only the selected samples\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Initialize parameters randomly\n",
    "    w = np.random.randn(n_features) * 0.01\n",
    "    b = 0.0\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Your code here\n",
    "        indices = np.random.choice(n_samples, batch_size, replace=False)\n",
    "        X_batch, y_batch = X[indices], y[indices]\n",
    "        \n",
    "        y_pred = predict(X_batch, w, b)\n",
    "        grad_w, grad_b = compute_gradients(X_batch, y_batch, y_pred)\n",
    "        \n",
    "        w = w - learning_rate * grad_w\n",
    "        b = b - learning_rate * grad_b\n",
    "        \n",
    "        loss = compute_mse(y_batch, y_pred)\n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        if verbose and (i % log_every_n_step == 0 or i == n_iterations - 1):\n",
    "            print(f\"Iteration {i:4d} | Loss: {loss:.6f}\")\n",
    "    \n",
    "    return w, b, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, loss_history = train_minibatch_gd(\n",
    "    X, y,\n",
    "    batch_size=64,\n",
    "    learning_rate=0.01,\n",
    "    n_iterations=200,\n",
    "    log_every_n_step=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Implement Learning Rate Scheduling\n",
    "\n",
    "Implement a training function that **decreases the learning rate** over time. This helps converge more precisely at the end of training.\n",
    "\n",
    "Common schedules:\n",
    "- Step decay: $\\alpha_t = \\alpha_0 \\cdot 0.9^{\\lfloor t/100 \\rfloor}$\n",
    "- Exponential decay: $\\alpha_t = \\alpha_0 \\cdot e^{-kt}$\n",
    "- Inverse time: $\\alpha_t = \\frac{\\alpha_0}{1 + k \\cdot t}$\n",
    "\n",
    "where $t$ is number of current step/iteration and $k$ is the decay constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_lr_schedule(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    initial_lr: float = 0.1,\n",
    "    schedule: str = 'exponential',  # 'step', 'exponential', or 'inverse'\n",
    "    n_iterations: int = 1000,\n",
    "    decay_constant: float = 0.0001,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train with learning rate scheduling.\n",
    "    \n",
    "    Implement at least one scheduling strategy.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.random.randn(n_features) * 0.01\n",
    "    b = 0.0\n",
    "    \n",
    "    learning_rate = initial_lr\n",
    "    loss_history = []\n",
    "    for i in range(n_iterations):\n",
    "        # Your code here\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test them all:\n",
    "print(\"Step decay:\")\n",
    "_, _, loss_history = train_with_lr_schedule(\n",
    "    X, y,\n",
    "    initial_lr=0.01,\n",
    "    schedule='step',\n",
    "    n_iterations=500,\n",
    "    decay_constant=0.0001\n",
    ")\n",
    "\n",
    "print(\"Exponential decay:\")\n",
    "_, _, loss_history = train_with_lr_schedule(\n",
    "    X, y,\n",
    "    initial_lr=0.01,\n",
    "    schedule='exponential',\n",
    "    n_iterations=500,\n",
    "    decay_constant=0.0001\n",
    ")\n",
    "\n",
    "print(\"Inverse time decay:\")\n",
    "_, _, loss_history = train_with_lr_schedule(\n",
    "    X, y,\n",
    "    initial_lr=0.01,\n",
    "    schedule='inverse',\n",
    "    n_iterations=500,\n",
    "    decay_constant=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Add Regularization (Ridge Regression)\n",
    "\n",
    "Implement **L2 regularization** (Ridge regression) to prevent overfitting.\n",
    "\n",
    "The loss function becomes:\n",
    "$$\\mathcal{L} = \\mathcal{L}_{MSE} + \\lambda \\sum w_i^2$$\n",
    "\n",
    "The gradient for weights becomes:\n",
    "$$\\frac{\\partial Loss}{\\partial w} = \\frac{\\partial MSE}{\\partial w} + 2\\lambda w$$\n",
    "\n",
    "where $\\lambda$ is the regularization constant and $w_i$ is the weight value of corresponding feature $i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ridge_loss(y_true: np.ndarray, y_pred: np.ndarray, w: np.ndarray, reg_lambda: float) -> float:\n",
    "    \"\"\"\n",
    "    Compute Ridge regression loss (MSE + L2 regularization).\n",
    "    \n",
    "    Args:\n",
    "        y_true: Actual target values\n",
    "        y_pred: Predicted values\n",
    "        w: Weight vector\n",
    "        reg_lambda: Regularization strength\n",
    "    \n",
    "    Returns:\n",
    "        Ridge loss value\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "def calculate_ridge_gradients(X: np.ndarray, y: np.ndarray, y_pred: np.ndarray, w: np.ndarray, reg_lambda: float) -> tuple:\n",
    "    \"\"\"\n",
    "    Compute gradients for Ridge regression.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: True target values\n",
    "        y_pred: Predicted values\n",
    "        w: Weight vector\n",
    "        reg_lambda: Regularization strength\n",
    "        \"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "def train_ridge_regression(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    learning_rate: float = 0.01,\n",
    "    reg_lambda: float = 0.1,  # Regularization strength\n",
    "    n_iterations: int = 1000\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train linear regression with L2 regularization.\n",
    "    \n",
    "    Hints:\n",
    "    - Modify the loss calculation to include regularization term\n",
    "    - Modify the gradient calculation for weights\n",
    "    - Note: We typically don't regularize the bias term\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _ =train_ridge_regression(\n",
    "    X, y,\n",
    "    learning_rate=0.01,\n",
    "    reg_lambda=0.1,\n",
    "    n_iterations=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Task: Implement Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Implement pure SGD where you update weights after **each individual sample** (batch_size=1).\n",
    "\n",
    "Compare the convergence behavior of:\n",
    "1. Batch GD (all samples)\n",
    "2. Mini-batch GD (e.g., 32 samples)\n",
    "3. SGD (1 sample)\n",
    "\n",
    "Plot the loss curves for all three on the same graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
